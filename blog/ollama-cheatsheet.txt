2:I[6485,["654","static/chunks/654-122e4f9245a087d8.js","576","static/chunks/576-560a11e2c7ce31a3.js","145","static/chunks/145-43e8020d63ee315f.js","797","static/chunks/app/blog/%5B...slug%5D/page-55ed13aca00f63be.js"],"Separator"]
3:I[4906,["654","static/chunks/654-122e4f9245a087d8.js","576","static/chunks/576-560a11e2c7ce31a3.js","145","static/chunks/145-43e8020d63ee315f.js","797","static/chunks/app/blog/%5B...slug%5D/page-55ed13aca00f63be.js"],"ZoomableImage"]
4:I[8852,["654","static/chunks/654-122e4f9245a087d8.js","576","static/chunks/576-560a11e2c7ce31a3.js","145","static/chunks/145-43e8020d63ee315f.js","797","static/chunks/app/blog/%5B...slug%5D/page-55ed13aca00f63be.js"],"CodeBlock"]
5:I[4707,[],""]
7:I[6423,[],""]
8:I[3162,["654","static/chunks/654-122e4f9245a087d8.js","648","static/chunks/648-e7080c8157c6c424.js","142","static/chunks/142-1683585cb4dcba15.js","617","static/chunks/617-14497bded1fb7fc4.js","576","static/chunks/576-560a11e2c7ce31a3.js","935","static/chunks/935-e3b3436437386019.js","185","static/chunks/app/layout-adecba16ebe90c45.js"],"default",1]
9:I[5292,["648","static/chunks/648-e7080c8157c6c424.js","160","static/chunks/app/not-found-e69b833f7545c4a9.js"],"default"]
6:["slug","ollama-cheatsheet","c"]
0:["e9ZQeCas_D-duamyMHGea",[[["",{"children":["blog",{"children":[["slug","ollama-cheatsheet","c"],{"children":["__PAGE__?{\"slug\":[\"ollama-cheatsheet\"]}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["blog",{"children":[["slug","ollama-cheatsheet","c"],{"children":["__PAGE__",{},[["$L1",["$","div",null,{"className":"relative min-h-screen flex","children":["$","article",null,{"className":"flex-1 mx-auto px-4 sm:px-6 lg:px-8 py-10 lg:py-20 max-w-3xl min-w-0 w-full overflow-hidden","children":[["$","div",null,{"className":"space-y-4","children":[["$","div",null,{"className":"flex flex-wrap gap-2 items-center text-sm text-muted-foreground mb-2","children":[["$","time",null,{"dateTime":"2025-01-29T00:00:00.000Z","children":"January 29, 2025"}],["$","span",null,{"children":"Â·"}],["$","span",null,{"children":[2," min read"]}]]}],["$","h1",null,{"className":"font-serif font-bold text-3xl sm:text-4xl md:text-5xl leading-tight break-words","children":"Tweaking GenAI LLMs locally using Ollama"}],["$","p",null,{"className":"text-lg sm:text-xl text-muted-foreground leading-relaxed break-words","children":"A quick reference guide for running GenAI LLM models and tweaking them using Ollama."}],["$","div",null,{"className":"flex flex-wrap gap-2 pt-4","children":[["$","div",null,{"className":"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 no-underline rounded-md","children":["code"," ",null]}],["$","div",null,{"className":"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 no-underline rounded-md","children":["blog"," ",null]}]]}]]}],["$","$L2",null,{"className":"my-8"}],["$","div",null,{"className":"prose prose-lg max-w-none min-w-0 w-full overflow-hidden prose-img:rounded-lg prose-img:max-w-full prose-img:h-auto prose-table:overflow-x-auto prose-table:block prose-table:w-full prose-headings:font-serif prose-headings:font-bold prose-headings:break-words prose-a:text-primary hover:prose-a:underline prose-a:break-words prose-p:break-words prose-p:overflow-wrap-break-word prose-p:word-break-break-word prose-li:break-words prose-li:overflow-wrap-break-word prose-h1:break-words prose-h2:break-words prose-h3:break-words  prose-h4:break-words prose-h5:break-words prose-h6:break-words prose-strong:break-words prose-code:break-words prose-code:text-wrap [&>*]:max-w-full [&>*]:overflow-hidden","children":[["$","h1",null,{"id":"what-is-ollama","children":["$","a",null,{"className":"subheading-anchor","aria-label":"Link to section","href":"#what-is-ollama","children":["$","strong",null,{"children":"What is Ollama"}]}]}],"\n",["$","p",null,{"children":"Ollama is an open-source platform that runs open LLMs on your local machine (or a server).\nIt acts like a bridge between any open LLM and your machine, not only running them but also providing an API layer on top of them so that another application or service can use them."}],"\n",["$","h1",null,{"id":"tweaking-genai-llm-locally-using-ollama","children":["$","a",null,{"className":"subheading-anchor","aria-label":"Link to section","href":"#tweaking-genai-llm-locally-using-ollama","children":["$","strong",null,{"children":"Tweaking GenAI LLM locally using Ollama"}]}]}],"\n",["$","p",null,{"children":"This guide provides essential commands and environment tweaks for optimizing Ollama when running complex LLM models locally."}],"\n",["$","h1",null,{"id":"prerequisites","children":["$","a",null,{"className":"subheading-anchor","aria-label":"Link to section","href":"#prerequisites","children":["$","strong",null,{"children":"Prerequisites"}]}]}],"\n",["$","p",null,{"children":["If you don't have Ollama installed, go to ",["$","a",null,{"href":"https://ollama.com","children":"https://ollama.com"}]," and download the installation package:"]}],"\n",["$","br",null,{}],"\n",["$","$L3",null,{"src":"/install_ollama.png"}],"\n",["$","br",null,{}],"\n",["$","h2",null,{"id":"running-a-model","children":["$","a",null,{"className":"subheading-anchor","aria-label":"Link to section","href":"#running-a-model","children":["$","strong",null,{"children":"Running a Model"}]}]}],"\n",["$","p",null,{"children":["Find your desired model on ",["$","a",null,{"href":"https://ollama.com","children":"https://ollama.com"}]," in this example we are going to use popular DeepSeek model:"]}],"\n",["$","br",null,{}],"\n",["$","$L3",null,{"src":"/ollama_model_page.png"}],"\n",["$","br",null,{}],"\n",["$","p",null,{"children":"To run our chosen model with Ollama open your terminal and run following command:"}],"\n",["$","figure",null,{"data-rehype-pretty-code-figure":"","children":["$","$L4",null,{"code":"ollama run deepseek-r1:14b","language":"text"}]}],"\n",["$","br",null,{}],"\n",["$","h2",null,{"id":"tweaking-model-parameters","children":["$","a",null,{"className":"subheading-anchor","aria-label":"Link to section","href":"#tweaking-model-parameters","children":["$","strong",null,{"children":"Tweaking Model Parameters"}]}]}],"\n",["$","br",null,{}],"\n",["$","p",null,{"children":"Ollama defaults model context size is set to 2048. To change this, you'll need to create a 'Modelfile' and include the following content"}],"\n",["$","br",null,{}],"\n",["$","figure",null,{"data-rehype-pretty-code-figure":"","children":["$","$L4",null,{"code":"FROM deepseek-r1:14b\n \nPARAMETER num_ctx 32768\nPARAMETER num_predict -1","language":"text"}]}],"\n",["$","br",null,{}],"\n",["$","p",null,{"children":"In this file, num_ctx is set to the maximum context size of the model, and num_predict is set to -1, which allows the model to use the remaining context as output context."}],"\n",["$","p",null,{"children":"After creating the file, run the following command:"}],"\n",["$","figure",null,{"data-rehype-pretty-code-figure":"","children":["$","$L4",null,{"code":"ollama create deepseek-r1-14b-32k-context -f ./Modelfile","language":"text"}]}],"\n",["$","p",null,{"children":"Now, you can use the entire context of the model without truncation by calling:"}],"\n",["$","figure",null,{"data-rehype-pretty-code-figure":"","children":["$","$L4",null,{"code":"ollama run deepseek-r1-14b-32k-context","language":"text"}]}],"\n",["$","h2",null,{"id":"performance-optimization","children":["$","a",null,{"className":"subheading-anchor","aria-label":"Link to section","href":"#performance-optimization","children":["$","strong",null,{"children":"Performance Optimization"}]}]}],"\n",["$","br",null,{}],"\n",["$","p",null,{"children":"Set environment variables to improve performance (maximize token generation speed):"}],"\n",["$","figure",null,{"data-rehype-pretty-code-figure":"","children":["$","$L4",null,{"code":"export OLLAMA_FLASH_ATTENTION=true \nexport OLLAMA_KV_CACHE_TYPE=f16    ","language":"text"}]}],"\n",["$","h3",null,{"id":"available-ollama_kv_cache_type-values","children":["$","a",null,{"className":"subheading-anchor","aria-label":"Link to section","href":"#available-ollama_kv_cache_type-values","children":["$","strong",null,{"children":["Available ",["$","code",null,{"children":"OLLAMA_KV_CACHE_TYPE"}]," Values"]}]}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","code",null,{"children":"q8_0"}]}],"\n",["$","li",null,{"children":["$","code",null,{"children":"q4_0"}]}],"\n",["$","li",null,{"children":[["$","code",null,{"children":"f16"}]," (for 16-bit floating point caching)"]}],"\n"]}],"\n",["$","br",null,{}],"\n",["$","p",null,{"children":"These settings can help optimize memory usage and speed when running large models locally."}]]}]]}]}],[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/9e092d2ffcb86e6f.css","precedence":"next","crossOrigin":"$undefined"}]]],null],null]},[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$6","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/b7758e6421324ed9.css","precedence":"next","crossOrigin":"$undefined"}]],["$","$L8",null,{"children":["$","$L5",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","$L9",null,{}],"notFoundStyles":[]}],"params":{}}]],null],null],["$La",null]]]]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Tweaking GenAI LLMs locally using Ollama"}],["$","meta","3",{"name":"description","content":"A quick reference guide for running GenAI LLM models and tweaking them using Ollama."}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
1:null
