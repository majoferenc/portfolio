<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4473ecc91f70f139-s.p.woff" as="font" crossorigin="" type="font/woff"/><link rel="preload" href="/_next/static/media/463dafcda517f24f-s.p.woff" as="font" crossorigin="" type="font/woff"/><link rel="stylesheet" href="/_next/static/css/f2151a86750c5367.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/54fc2249fbc04998.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-f9ff335114d0509a.js"/><script src="/_next/static/chunks/fd9d1056-0695158323f273ad.js" async=""></script><script src="/_next/static/chunks/117-33c02dcf674d4f63.js" async=""></script><script src="/_next/static/chunks/main-app-aeede251ade49f84.js" async=""></script><script src="/_next/static/chunks/972-7d8061f2d2c7bb54.js" async=""></script><script src="/_next/static/chunks/590-19b69f1987fe1595.js" async=""></script><script src="/_next/static/chunks/949-7217ba085a9645be.js" async=""></script><script src="/_next/static/chunks/925-078a6aad6f770627.js" async=""></script><script src="/_next/static/chunks/app/layout-2e0e0c4d4a716cff.js" async=""></script><script src="/_next/static/chunks/137-9598eab4ff025283.js" async=""></script><script src="/_next/static/chunks/app/blog/page-87f8932c7badbef8.js" async=""></script><script src="/_next/static/chunks/274-36a2e15302fd524b.js" async=""></script><script src="/_next/static/chunks/541-9307a56f681ede6e.js" async=""></script><script src="/_next/static/chunks/app/blog/%5B...slug%5D/page-17eba02d470a61f2.js" async=""></script><title>Tweaking GenAI LLMs locally using Ollama</title><meta name="description" content="A quick reference guide for running GenAI LLM models and tweaking them using Ollama."/><meta name="author" content="MarianFerenc"/><meta property="og:title" content="Tweaking GenAI LLMs locally using Ollama"/><meta property="og:description" content="A quick reference guide for running GenAI LLM models and tweaking them using Ollama."/><meta property="og:url" content="blog/ollama-cheatsheet"/><meta property="og:type" content="article"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Tweaking GenAI LLMs locally using Ollama"/><meta name="twitter:description" content="A quick reference guide for running GenAI LLM models and tweaking them using Ollama."/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_1e4310 __variable_c3aa02 antialiased min-h-screen flex flex-col"><span><div class="absolute min-h-screen flex -z-40"><div class="sidebar bg-black text-white w-64 space-y-6 py-7 fixed px-2 inset-y-0 -translate-x-full left-0 transform transition duration-200 ease-in-out -translate-x-full"><div class="text-white flex items-center space-x-2 px-4 justify-between"><span class="text-sm text-white font-bold w-fit">Ing. Mari√°n Ferenc</span><button type="button"><svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path></svg></button></div><nav><a class="block py-2.5 px-4 rounded transition duration-200 hover:bg-yellow-500 text-white font-bold" href="/">Home</a><a class="block py-2.5 px-4 rounded transition duration-200 hover:bg-yellow-500 text-white font-bold" href="/blog">Blog</a><a class="block py-2.5 px-4 rounded transition duration-200 hover:bg-yellow-500 text-white font-bold" href="/contact">Contact</a></nav></div></div><div><header class="flex w-auto bg-black bg-opacity-50 fixed inset-x-0 z-30 shadow"><div class="flex"><button type="submit" class="mobile-menu-button p-4 focus:outline-none focus:bg-blue-800"><svg class="h-5 w-5 z-30" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="white"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button><div class="flex flex-wrap place-content-center h-16"><a href="/"><div class="shadow text-white font-medium text-lg">Ing. Marian Ferenc</div></a></div></div></header></div><div class="fixed inset-0 pointer-events-none z-[9999] mix-blend-difference" style="mask-image:radial-gradient(circle 5rem at var(--mouse-x) var(--mouse-y), white 100%, transparent 100%);-webkit-mask-image:radial-gradient(circle 2rem at var(--mouse-x) var(--mouse-y), white 100%, transparent 100%)"><div class="w-full h-full bg-white"></div></div><div class="fixed w-4 h-4 border-2 border-black rounded-full pointer-events-none z-[10000] mix-blend-hue" style="left:0px;top:0px;transform:translate(-50%, -50%)"></div></span><div class="flex-grow"><article class="py-20 dark:prose-invert max-w-3xl mx-auto h-full max-w-5xl max-h-full px-10"><h1 class="mb-2 text-4xl font-bold">Tweaking GenAI LLMs locally using Ollama</h1><div class="flex gap-2 mb-2"><a class="inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 no-underline rounded-md" href="/blog/tags/code">code<!-- --> </a><a class="inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 no-underline rounded-md" href="/blog/tags/blog">blog<!-- --> </a></div><p class="text-xl mt-0 text-orange-400">A quick reference guide for running GenAI LLM models and tweaking them using Ollama.</p><hr class="my-4"/><h1 id="what-is-ollama"><a class="subheading-anchor" aria-label="Link to section" href="#what-is-ollama"><strong>What is Ollama</strong></a></h1>
<p>Ollama is an open-source platform that runs open LLMs on your local machine (or a server).
It acts like a bridge between any open LLM and your machine, not only running them but also providing an API layer on top of them so that another application or service can use them.</p>
<h1 id="tweaking-genai-llm-locally-using-ollama"><a class="subheading-anchor" aria-label="Link to section" href="#tweaking-genai-llm-locally-using-ollama"><strong>Tweaking GenAI LLM locally using Ollama</strong></a></h1>
<p>This guide provides essential commands and environment tweaks for optimizing Ollama when running complex LLM models locally.</p>
<h1 id="prerequisites"><a class="subheading-anchor" aria-label="Link to section" href="#prerequisites"><strong>Prerequisites</strong></a></h1>
<p>If you don&#x27;t have Ollama installed, go to <a href="https://ollama.com">https://ollama.com</a> and download the installation package:</p>
<br/>
<img loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="cursor-zoom-in rounded-lg transition-transform hover:scale-[1.02]" style="color:transparent" src="/install_ollama.png"/>
<br/>
<h2 id="running-a-model"><a class="subheading-anchor" aria-label="Link to section" href="#running-a-model"><strong>Running a Model</strong></a></h2>
<p>Find your desired model on <a href="https://ollama.com">https://ollama.com</a> in this example we are going to use popular DeepSeek model:</p>
<br/>
<img loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="cursor-zoom-in rounded-lg transition-transform hover:scale-[1.02]" style="color:transparent" src="/ollama_model_page.png"/>
<br/>
<p>To run our chosen model with Ollama open your terminal and run following command:</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="bash" data-theme="github-dark"><code data-language="bash" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#B392F0">ollama</span><span style="color:#9ECBFF"> run</span><span style="color:#9ECBFF"> deepseek-r1:14b</span></span></code></pre></figure>
<br/>
<h2 id="tweaking-model-parameters"><a class="subheading-anchor" aria-label="Link to section" href="#tweaking-model-parameters"><strong>Tweaking Model Parameters</strong></a></h2>
<br/>
<p>Ollama defaults model context size is set to 2048. To change this, you&#x27;ll need to create a &#x27;Modelfile&#x27; and include the following content</p>
<br/>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="ollama" data-theme="github-dark"><code data-language="ollama" data-theme="github-dark" style="display:grid"><span data-line=""><span>FROM deepseek-r1:14b</span></span>
<span data-line=""> </span>
<span data-line=""><span>PARAMETER num_ctx 32768</span></span>
<span data-line=""><span>PARAMETER num_predict -1</span></span></code></pre></figure>
<br/>
<p>In this file, num_ctx is set to the maximum context size of the model, and num_predict is set to -1, which allows the model to use the remaining context as output context.</p>
<p>After creating the file, run the following command:</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="bash" data-theme="github-dark"><code data-language="bash" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#B392F0">ollama</span><span style="color:#9ECBFF"> create</span><span style="color:#9ECBFF"> deepseek-r1-14b-32k-context</span><span style="color:#79B8FF"> -f</span><span style="color:#9ECBFF"> ./Modelfile</span></span></code></pre></figure>
<p>Now, you can use the entire context of the model without truncation by calling:</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="bash" data-theme="github-dark"><code data-language="bash" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#B392F0">ollama</span><span style="color:#9ECBFF"> run</span><span style="color:#9ECBFF"> deepseek-r1-14b-32k-context</span></span></code></pre></figure>
<h2 id="performance-optimization"><a class="subheading-anchor" aria-label="Link to section" href="#performance-optimization"><strong>Performance Optimization</strong></a></h2>
<br/>
<p>Set environment variables to improve performance (maximize token generation speed):</p>
<figure data-rehype-pretty-code-figure=""><pre style="background-color:#24292e;color:#e1e4e8" tabindex="0" data-language="bash" data-theme="github-dark"><code data-language="bash" data-theme="github-dark" style="display:grid"><span data-line=""><span style="color:#F97583">export</span><span style="color:#E1E4E8"> OLLAMA_FLASH_ATTENTION</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">true </span></span>
<span data-line=""><span style="color:#F97583">export</span><span style="color:#E1E4E8"> OLLAMA_KV_CACHE_TYPE</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">f16    </span></span></code></pre></figure>
<h3 id="available-ollama_kv_cache_type-values"><a class="subheading-anchor" aria-label="Link to section" href="#available-ollama_kv_cache_type-values"><strong>Available <code>OLLAMA_KV_CACHE_TYPE</code> Values</strong></a></h3>
<ul>
<li><code>q8_0</code></li>
<li><code>q4_0</code></li>
<li><code>f16</code> (for 16-bit floating point caching)</li>
</ul>
<br/>
<p>These settings can help optimize memory usage and speed when running large models locally.</p></article></div><div><footer class="bg-black"><section class="relative grid place-content-center overflow-hidden bg-black px-4 py-5 text-gray-200" style="background-image:radial-gradient(125% 125% at 50% 0%, #020617 50% #FFEDCC)"><div class="relative z-10 flex flex-col items-center"><div class="grid grid-cols-1 gap-8"><p class="max-w-xl text-center text-base leading-relaxed md:text-lg md:leading-relaxed">Ing. Marian Ferenc - <!-- -->2025</p></div></div></section></footer></div><script src="/_next/static/chunks/webpack-f9ff335114d0509a.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/4473ecc91f70f139-s.p.woff\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff\"}]\n2:HL[\"/_next/static/media/463dafcda517f24f-s.p.woff\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff\"}]\n3:HL[\"/_next/static/css/f2151a86750c5367.css\",\"style\"]\n4:HL[\"/_next/static/css/54fc2249fbc04998.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[2846,[],\"\"]\n8:I[4707,[],\"\"]\na:I[6423,[],\"\"]\nb:I[1258,[\"972\",\"static/chunks/972-7d8061f2d2c7bb54.js\",\"590\",\"static/chunks/590-19b69f1987fe1595.js\",\"949\",\"static/chunks/949-7217ba085a9645be.js\",\"925\",\"static/chunks/925-078a6aad6f770627.js\",\"185\",\"static/chunks/app/layout-2e0e0c4d4a716cff.js\"],\"default\",1]\nd:I[1060,[],\"\"]\n9:[\"slug\",\"ollama-cheatsheet\",\"c\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L5\",null,{\"buildId\":\"MAMxe3hVp87S1-IRlHRsf\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"ollama-cheatsheet\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"ollama-cheatsheet\",\"c\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":[\\\"ollama-cheatsheet\\\"]}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"ollama-cheatsheet\",\"c\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",\"$L7\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/54fc2249fbc04998.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/f2151a86750c5367.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}],\"params\":{}}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]\n"])</script><script>self.__next_f.push([1,"f:I[2972,[\"137\",\"static/chunks/137-9598eab4ff025283.js\",\"972\",\"static/chunks/972-7d8061f2d2c7bb54.js\",\"404\",\"static/chunks/app/blog/page-87f8932c7badbef8.js\"],\"\"]\n10:I[1553,[\"137\",\"static/chunks/137-9598eab4ff025283.js\",\"972\",\"static/chunks/972-7d8061f2d2c7bb54.js\",\"274\",\"static/chunks/274-36a2e15302fd524b.js\",\"541\",\"static/chunks/541-9307a56f681ede6e.js\",\"797\",\"static/chunks/app/blog/%5B...slug%5D/page-17eba02d470a61f2.js\"],\"ZoomableImage\"]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"article\",null,{\"className\":\"py-20 dark:prose-invert max-w-3xl mx-auto h-full max-w-5xl max-h-full px-10\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-2 text-4xl font-bold\",\"children\":\"Tweaking GenAI LLMs locally using Ollama\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-2 mb-2\",\"children\":[[\"$\",\"$Lf\",null,{\"className\":\"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 no-underline rounded-md\",\"href\":\"/blog/tags/code\",\"children\":[\"code\",\" \",null]}],[\"$\",\"$Lf\",null,{\"className\":\"inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 no-underline rounded-md\",\"href\":\"/blog/tags/blog\",\"children\":[\"blog\",\" \",null]}]]}],[\"$\",\"p\",null,{\"className\":\"text-xl mt-0 text-orange-400\",\"children\":\"A quick reference guide for running GenAI LLM models and tweaking them using Ollama.\"}],[\"$\",\"hr\",null,{\"className\":\"my-4\"}],[[\"$\",\"h1\",null,{\"id\":\"what-is-ollama\",\"children\":[\"$\",\"a\",null,{\"className\":\"subheading-anchor\",\"aria-label\":\"Link to section\",\"href\":\"#what-is-ollama\",\"children\":[\"$\",\"strong\",null,{\"children\":\"What is Ollama\"}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Ollama is an open-source platform that runs open LLMs on your local machine (or a server).\\nIt acts like a bridge between any open LLM and your machine, not only running them but also providing an API layer on top of them so that another application or service can use them.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"tweaking-genai-llm-locally-using-ollama\",\"children\":[\"$\",\"a\",null,{\"className\":\"subheading-anchor\",\"aria-label\":\"Link to section\",\"href\":\"#tweaking-genai-llm-locally-using-ollama\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Tweaking GenAI LLM locally using Ollama\"}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This guide provides essential commands and environment tweaks for optimizing Ollama when running complex LLM models locally.\"}],\"\\n\",[\"$\",\"h1\",null,{\"id\":\"prerequisites\",\"children\":[\"$\",\"a\",null,{\"className\":\"subheading-anchor\",\"aria-label\":\"Link to section\",\"href\":\"#prerequisites\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Prerequisites\"}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"If you don't have Ollama installed, go to \",[\"$\",\"a\",null,{\"href\":\"https://ollama.com\",\"children\":\"https://ollama.com\"}],\" and download the installation package:\"]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"$L10\",null,{\"src\":\"/install_ollama.png\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"running-a-model\",\"children\":[\"$\",\"a\",null,{\"className\":\"subheading-anchor\",\"aria-label\":\"Link to section\",\"href\":\"#running-a-model\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Running a Model\"}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Find your desired model on \",[\"$\",\"a\",null,{\"href\":\"https://ollama.com\",\"children\":\"https://ollama.com\"}],\" in this example we are going to use popular DeepSeek model:\"]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"$L10\",null,{\"src\":\"/ollama_model_page.png\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"To run our chosen model with Ollama open your terminal and run following command:\"}],\"\\n\",[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\"ollama\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" run\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" deepseek-r1:14b\"}]]}]}]}]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"tweaking-model-parameters\",\"children\":[\"$\",\"a\",null,{\"className\":\"subheading-anchor\",\"aria-label\":\"Link to section\",\"href\":\"#tweaking-model-parameters\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Tweaking Model Parameters\"}]}]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Ollama defaults model context size is set to 2048. To change this, you'll need to create a 'Modelfile' and include the following content\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"ollama\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"ollama\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"children\":\"FROM deepseek-r1:14b\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":\" \"}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"children\":\"PARAMETER num_ctx 32768\"}]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[\"$\",\"span\",null,{\"children\":\"PARAMETER num_predict -1\"}]}]]}]}]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In this file, num_ctx is set to the maximum context size of the model, and num_predict is set to -1, which allows the model to use the remaining context as output context.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"After creating the file, run the following command:\"}],\"\\n\",[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\"ollama\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" create\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" deepseek-r1-14b-32k-context\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#79B8FF\"},\"children\":\" -f\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" ./Modelfile\"}]]}]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Now, you can use the entire context of the model without truncation by calling:\"}],\"\\n\",[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#B392F0\"},\"children\":\"ollama\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" run\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#9ECBFF\"},\"children\":\" deepseek-r1-14b-32k-context\"}]]}]}]}]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"performance-optimization\",\"children\":[\"$\",\"a\",null,{\"className\":\"subheading-anchor\",\"aria-label\":\"Link to section\",\"href\":\"#performance-optimization\",\"children\":[\"$\",\"strong\",null,{\"children\":\"Performance Optimization\"}]}]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Set environment variables to improve performance (maximize token generation speed):\"}],\"\\n\",[\"$\",\"figure\",null,{\"data-rehype-pretty-code-figure\":\"\",\"children\":[\"$\",\"pre\",null,{\"style\":{\"backgroundColor\":\"#24292e\",\"color\":\"#e1e4e8\"},\"tabIndex\":\"0\",\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"children\":[\"$\",\"code\",null,{\"data-language\":\"bash\",\"data-theme\":\"github-dark\",\"style\":{\"display\":\"grid\"},\"children\":[[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"export\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" OLLAMA_FLASH_ATTENTION\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"true \"}]]}],\"\\n\",[\"$\",\"span\",null,{\"data-line\":\"\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"export\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\" OLLAMA_KV_CACHE_TYPE\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"color\":\"#E1E4E8\"},\"children\":\"f16    \"}]]}]]}]}]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"available-ollama_kv_cache_type-values\",\"children\":[\"$\",\"a\",null,{\"className\":\"subheading-anchor\",\"aria-label\":\"Link to section\",\"href\":\"#available-ollama_kv_cache_type-values\",\"children\":[\"$\",\"strong\",null,{\"children\":[\"Available \",[\"$\",\"code\",null,{\"children\":\"OLLAMA_KV_CACHE_TYPE\"}],\" Values\"]}]}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"q8_0\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"q4_0\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"f16\"}],\" (for 16-bit floating point caching)\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"These settings can help optimize memory usage and speed when running large models locally.\"}]]]}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Tweaking GenAI LLMs locally using Ollama\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"A quick reference guide for running GenAI LLM models and tweaking them using Ollama.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"MarianFerenc\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Tweaking GenAI LLMs locally using Ollama\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:description\",\"content\":\"A quick reference guide for running GenAI LLM models and tweaking them using Ollama.\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:url\",\"content\":\"blog/ollama-cheatsheet\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Tweaking GenAI LLMs locally using Ollama\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"A quick reference guide for running GenAI LLM models and tweaking them using Ollama.\"}],[\"$\",\"link\",\"12\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"13\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script></body></html>