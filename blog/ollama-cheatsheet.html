<!DOCTYPE html><!--7N4vOeyKQXYIVFJChpA_p--><html lang="en" data-theme="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/GeistMonoVF-s.p.2cee7d16.woff" as="font" crossorigin="" type="font/woff"/><link rel="preload" href="/_next/static/media/GeistVF-s.p.4c3c0b96.woff" as="font" crossorigin="" type="font/woff"/><link rel="stylesheet" href="/_next/static/chunks/57dacccb0e5ac45e.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/f78c28c3218a0a43.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/5398e9fba21119c2.js"/><script src="/_next/static/chunks/15ad49dc5c86d7e5.js" async=""></script><script src="/_next/static/chunks/edd44c3b698fb063.js" async=""></script><script src="/_next/static/chunks/3d9f13b791d68e15.js" async=""></script><script src="/_next/static/chunks/turbopack-0abe05699d3eb167.js" async=""></script><script src="/_next/static/chunks/bf74061b5f5e5c25.js" async=""></script><script src="/_next/static/chunks/33d9c0c0b8c2a097.js" async=""></script><script src="/_next/static/chunks/80e392c6d1303c65.js" async=""></script><script src="/_next/static/chunks/e3b77110d15b3613.js" async=""></script><script src="/_next/static/chunks/4530abaaf272c463.js" async=""></script><script src="/_next/static/chunks/4954f07e7ecfb4a1.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/b71c1cfea9076b4b.js" async=""></script><script src="/_next/static/chunks/64f54b9562f6597b.js" async=""></script><script src="/_next/static/chunks/b632cf93e0d36000.js" async=""></script><meta name="next-size-adjust" content=""/><title>Tweaking GenAI LLMs locally using Ollama</title><meta name="description" content="A quick reference guide for running GenAI LLM models and tweaking them using Ollama."/><link rel="icon" href="/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="geistsans_47a3c9f1-module__MCSGAW__variable geistmono_cfbefb1d-module__fDTvyq__variable antialiased min-h-screen flex flex-col neural-grid"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col items-center justify-center"><canvas class="absolute inset-0 z-0 w-full h-full" id="canvas" style="width:100%;height:100%"></canvas><div class="absolute inset-0 w-full h-full -z-10"></div></div><header class="fixed top-3.5 left-1/2 -translate-x-1/2 z-50 transition-all duration-300 rounded-full border border-white/10 backdrop-blur-4xl
        h-14 bg-stone-800 w-[95%] max-w-5xl"><div class="mx-auto h-full px-6"><nav class="flex items-center justify-between h-full"><div class="flex items-center gap-3"><button class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-10 w-10 glass md:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu h-5 w-5"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><a class="font-bold text-base text-white" href="/">Ing. Marian Ferenc</a></div><div class="hidden md:flex items-center gap-6"><span class="w-px h-5 bg-gray-500/50"></span><a class="text-sm transition-all duration-300 text-gray-300 hover:text-white" href="/blog">Blog</a><a class="text-sm transition-all duration-300 text-gray-300 hover:text-white" href="/contact">Contact</a></div></nav></div></header><!--$?--><template id="B:0"></template><div>Loading...</div><!--/$--><div><footer><section class="relative bg-black rounded-tl-[120rem] min-h-[200px] grid place-content-center overflow-hidden bg-black px-4 py-5 text-gray-200" style="background-image:radial-gradient(125% 125% at 50% 0%, #020617 50% #FFEDCC)"><div class="relative z-10 flex flex-col items-center"><div class="grid grid-cols-1 gap-8"><div class="max-w-xl text-center text-base leading-relaxed md:text-lg md:leading-relaxed"><p class="font-medium">© Ing. Marian Ferenc - <!-- -->2025</p><p>All rights reserved</p></div></div></div></section></footer></div><script>requestAnimationFrame(function(){$RT=performance.now()});</script><script src="/_next/static/chunks/5398e9fba21119c2.js" id="_R_" async=""></script><div hidden id="S:0"><div class="flex-grow relative z-10 transition-all duration-300 "><div class="relative min-h-screen flex bg-background"><article class="flex-1 mx-auto px-4 sm:px-6 lg:px-8 py-10 lg:py-20 max-w-4xl min-w-0 w-full overflow-hidden"><div class="space-y-4 mb-8"><div class="flex flex-wrap gap-2 items-center text-sm text-muted-foreground mb-4"><time dateTime="Wed Jan 29 2025 00:00:00 GMT+0000 (Coordinated Universal Time)">January 29, 2025</time><span>·</span><span>2<!-- --> min read</span></div><div class="flex flex-wrap gap-2 pt-4"><span class="inline-flex items-center px-3 py-1 rounded-full text-sm bg-gray-800 text-gray-300 border border-gray-700">code</span><span class="inline-flex items-center px-3 py-1 rounded-full text-sm bg-gray-800 text-gray-300 border border-gray-700">blog</span></div></div><div data-orientation="horizontal" role="none" class="shrink-0 h-[1px] w-full my-8 bg-gray-700"></div><div class="prose prose-lg max-w-none min-w-0 w-full overflow-hidden"><h1><strong>What is Ollama</strong></h1>
<p>Ollama is an open-source platform that runs open LLMs on your local machine (or a server).
It acts like a bridge between any open LLM and your machine, not only running them but also providing an API layer on top of them so that another application or service can use them.</p>
<h1><strong>Tweaking GenAI LLM locally using Ollama</strong></h1>
<p>This guide provides essential commands and environment tweaks for optimizing Ollama when running complex LLM models locally.</p>
<h1><strong>Prerequisites</strong></h1>
<p>If you don&#x27;t have Ollama installed, go to https://ollama.com and download the installation package:</p>
<br/>
<img loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="cursor-zoom-in rounded-lg transition-transform hover:scale-[1.02]" style="color:transparent" src="/install_ollama.png"/>
<br/>
<h2><strong>Running a Model</strong></h2>
<p>Find your desired model on https://ollama.com in this example we are going to use popular DeepSeek model:</p>
<br/>
<img loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="cursor-zoom-in rounded-lg transition-transform hover:scale-[1.02]" style="color:transparent" src="/ollama_model_page.png"/>
<br/>
<p>To run our chosen model with Ollama open your terminal and run following command:</p>
<div class="relative group my-4 w-full max-w-full overflow-hidden bg-black px-2"><div class="overflow-x-auto max-w-full"><pre class="rounded-lg p-4 bg-muted text-sm whitespace-pre overflow-x-auto max-w-full block"><code class="language-bash block text-white">ollama run deepseek-r1:14b
</code></pre></div><button class="absolute top-2 right-2 p-1 rounded bg-background/80  hover:bg-background transition-colors z-10" aria-label="Copy code"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy h-4 w-4 text-muted-foreground"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div>
<br/>
<h2><strong>Tweaking Model Parameters</strong></h2>
<br/>
<p>Ollama defaults model context size is set to 2048. To change this, you&#x27;ll need to create a &#x27;Modelfile&#x27; and include the following content</p>
<br/>
<div class="relative group my-4 w-full max-w-full overflow-hidden bg-black px-2"><div class="overflow-x-auto max-w-full"><pre class="rounded-lg p-4 bg-muted text-sm whitespace-pre overflow-x-auto max-w-full block"><code class="language-ollama block text-white">FROM deepseek-r1:14b

PARAMETER num_ctx 32768
PARAMETER num_predict -1
</code></pre></div><button class="absolute top-2 right-2 p-1 rounded bg-background/80  hover:bg-background transition-colors z-10" aria-label="Copy code"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy h-4 w-4 text-muted-foreground"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div>
<br/>
<p>In this file, num_ctx is set to the maximum context size of the model, and num_predict is set to -1, which allows the model to use the remaining context as output context.</p>
<p>After creating the file, run the following command:</p>
<div class="relative group my-4 w-full max-w-full overflow-hidden bg-black px-2"><div class="overflow-x-auto max-w-full"><pre class="rounded-lg p-4 bg-muted text-sm whitespace-pre overflow-x-auto max-w-full block"><code class="language-bash block text-white">ollama create deepseek-r1-14b-32k-context -f ./Modelfile
</code></pre></div><button class="absolute top-2 right-2 p-1 rounded bg-background/80  hover:bg-background transition-colors z-10" aria-label="Copy code"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy h-4 w-4 text-muted-foreground"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div>
<p>Now, you can use the entire context of the model without truncation by calling:</p>
<div class="relative group my-4 w-full max-w-full overflow-hidden bg-black px-2"><div class="overflow-x-auto max-w-full"><pre class="rounded-lg p-4 bg-muted text-sm whitespace-pre overflow-x-auto max-w-full block"><code class="language-bash block text-white">ollama run deepseek-r1-14b-32k-context
</code></pre></div><button class="absolute top-2 right-2 p-1 rounded bg-background/80  hover:bg-background transition-colors z-10" aria-label="Copy code"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy h-4 w-4 text-muted-foreground"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div>
<h2><strong>Performance Optimization</strong></h2>
<br/>
<p>Set environment variables to improve performance (maximize token generation speed):</p>
<div class="relative group my-4 w-full max-w-full overflow-hidden bg-black px-2"><div class="overflow-x-auto max-w-full"><pre class="rounded-lg p-4 bg-muted text-sm whitespace-pre overflow-x-auto max-w-full block"><code class="language-bash block text-white">export OLLAMA_FLASH_ATTENTION=true 
export OLLAMA_KV_CACHE_TYPE=f16    
</code></pre></div><button class="absolute top-2 right-2 p-1 rounded bg-background/80  hover:bg-background transition-colors z-10" aria-label="Copy code"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-copy h-4 w-4 text-muted-foreground"><rect width="14" height="14" x="8" y="8" rx="2" ry="2"></rect><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"></path></svg></button></div>
<h3><strong>Available <code>OLLAMA_KV_CACHE_TYPE</code> Values</strong></h3>
<ul>
<li><code>q8_0</code></li>
<li><code>q4_0</code></li>
<li><code>f16</code> (for 16-bit floating point caching)</li>
</ul>
<br/>
<p>These settings can help optimize memory usage and speed when running large models locally.</p></div><div class="mt-16 pt-8 border-t border-gray-800"><a class="inline-flex items-center text-yellow-400 hover:text-yellow-300 transition-colors" href="/blog">← Back to Blog</a></div></article></div><!--$--><!--/$--></div></div><script>$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[27423,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\"],\"ThemeProvider\"]\n3:I[31080,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\"],\"WavyBackground\"]\n4:I[37196,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\"],\"default\"]\n5:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/b71c1cfea9076b4b.js\"],\"default\"]\n6:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/b71c1cfea9076b4b.js\"],\"default\"]\n7:I[29306,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\",\"/_next/static/chunks/64f54b9562f6597b.js\"],\"default\"]\n8:I[24415,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\"],\"Footer\"]\na:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/b71c1cfea9076b4b.js\"],\"OutletBoundary\"]\nb:\"$Sreact.suspense\"\nd:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/b71c1cfea9076b4b.js\"],\"ViewportBoundary\"]\nf:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/b71c1cfea9076b4b.js\"],\"MetadataBoundary\"]\n11:I[68027,[],\"default\"]\n:HL[\"/_next/static/chunks/57dacccb0e5ac45e.css\",\"style\"]\n:HL[\"/_next/static/media/GeistMonoVF-s.p.2cee7d16.woff\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff\"}]\n:HL[\"/_next/static/media/GeistVF-s.p.4c3c0b96.woff\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff\"}]\n:HL[\"/_next/static/chunks/f78c28c3218a0a43.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"7N4vOeyKQXYIVFJChpA_p\",\"c\":[\"\",\"blog\",\"ollama-cheatsheet\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"ollama-cheatsheet\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/57dacccb0e5ac45e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/80e392c6d1303c65.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/e3b77110d15b3613.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-4\",{\"src\":\"/_next/static/chunks/4530abaaf272c463.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-5\",{\"src\":\"/_next/static/chunks/4954f07e7ecfb4a1.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"data-theme\":\"dark\",\"children\":[\"$\",\"body\",null,{\"className\":\"geistsans_47a3c9f1-module__MCSGAW__variable geistmono_cfbefb1d-module__fDTvyq__variable antialiased min-h-screen flex flex-col neural-grid\",\"children\":[\"$\",\"$L2\",null,{\"defaultTheme\":\"dark\",\"storageKey\":\"theme-mode\",\"children\":[[\"$\",\"$L3\",null,{\"className\":\"absolute inset-0 w-full h-full -z-10\"}],[\"$\",\"$L4\",null,{\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"$L7\",null,{}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L8\",null,{}]]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L9\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/f78c28c3218a0a43.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/b632cf93e0d36000.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$La\",null,{\"children\":[\"$\",\"$b\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@c\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$Ld\",null,{\"children\":\"$@e\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lf\",null,{\"children\":[\"$\",\"$b\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@10\"}]}]}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:I[72436,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\",\"/_next/static/chunks/b632cf93e0d36000.js\"],\"Separator\"]\n14:I[22016,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\",\"/_next/static/chunks/b632cf93e0d36000.js\"],\"\"]\n9:[\"$\",\"div\",null,{\"className\":\"relative min-h-screen flex bg-background\",\"children\":[\"$\",\"article\",null,{\"className\":\"flex-1 mx-auto px-4 sm:px-6 lg:px-8 py-10 lg:py-20 max-w-4xl min-w-0 w-full overflow-hidden\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-y-4 mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 items-center text-sm text-muted-foreground mb-4\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"$D2025-01-29T00:00:00.000Z\",\"children\":\"January 29, 2025\"}],[\"$\",\"span\",null,{\"children\":\"·\"}],[\"$\",\"span\",null,{\"children\":[2,\" min read\"]}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 pt-4\",\"children\":[[\"$\",\"span\",\"code\",{\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm bg-gray-800 text-gray-300 border border-gray-700\",\"children\":\"code\"}],[\"$\",\"span\",\"blog\",{\"className\":\"inline-flex items-center px-3 py-1 rounded-full text-sm bg-gray-800 text-gray-300 border border-gray-700\",\"children\":\"blog\"}]]}]]}],[\"$\",\"$L12\",null,{\"className\":\"my-8 bg-gray-700\"}],[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none min-w-0 w-full overflow-hidden\",\"children\":\"$L13\"}],[\"$\",\"div\",null,{\"className\":\"mt-16 pt-8 border-t border-gray-800\",\"children\":[\"$\",\"$L14\",null,{\"href\":\"/blog\",\"className\":\"inline-flex items-center text-yellow-400 hover:text-yellow-300 transition-colors\",\"children\":\"← Back to Blog\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"15:I[46682,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\",\"/_next/static/chunks/b632cf93e0d36000.js\"],\"ZoomableImage\"]\n16:I[66828,[\"/_next/static/chunks/bf74061b5f5e5c25.js\",\"/_next/static/chunks/33d9c0c0b8c2a097.js\",\"/_next/static/chunks/80e392c6d1303c65.js\",\"/_next/static/chunks/e3b77110d15b3613.js\",\"/_next/static/chunks/4530abaaf272c463.js\",\"/_next/static/chunks/4954f07e7ecfb4a1.js\",\"/_next/static/chunks/b632cf93e0d36000.js\"],\"CodeBlock\"]\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"h1\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"What is Ollama\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Ollama is an open-source platform that runs open LLMs on your local machine (or a server).\\nIt acts like a bridge between any open LLM and your machine, not only running them but also providing an API layer on top of them so that another application or service can use them.\"}],\"\\n\",[\"$\",\"h1\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Tweaking GenAI LLM locally using Ollama\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This guide provides essential commands and environment tweaks for optimizing Ollama when running complex LLM models locally.\"}],\"\\n\",[\"$\",\"h1\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Prerequisites\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"If you don't have Ollama installed, go to https://ollama.com and download the installation package:\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"$L15\",null,{\"src\":\"/install_ollama.png\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Running a Model\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Find your desired model on https://ollama.com in this example we are going to use popular DeepSeek model:\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"$L15\",null,{\"src\":\"/ollama_model_page.png\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"To run our chosen model with Ollama open your terminal and run following command:\"}],\"\\n\",[\"$\",\"$L16\",null,{\"code\":\"ollama run deepseek-r1:14b\\n\",\"language\":\"bash\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"h2\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Tweaking Model Parameters\"}]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Ollama defaults model context size is set to 2048. To change this, you'll need to create a 'Modelfile' and include the following content\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"$L16\",null,{\"code\":\"FROM deepseek-r1:14b\\n\\nPARAMETER num_ctx 32768\\nPARAMETER num_predict -1\\n\",\"language\":\"ollama\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In this file, num_ctx is set to the maximum context size of the model, and num_predict is set to -1, which allows the model to use the remaining context as output context.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"After creating the file, run the following command:\"}],\"\\n\",[\"$\",\"$L16\",null,{\"code\":\"ollama create deepseek-r1-14b-32k-context -f ./Modelfile\\n\",\"language\":\"bash\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Now, you can use the entire context of the model without truncation by calling:\"}],\"\\n\",[\"$\",\"$L16\",null,{\"code\":\"ollama run deepseek-r1-14b-32k-context\\n\",\"language\":\"bash\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Performance Optimization\"}]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Set environment variables to improve performance (maximize token generation speed):\"}],\"\\n\",[\"$\",\"$L16\",null,{\"code\":\"export OLLAMA_FLASH_ATTENTION=true \\nexport OLLAMA_KV_CACHE_TYPE=f16    \\n\",\"language\":\"bash\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":[\"Available \",[\"$\",\"code\",null,{\"children\":\"OLLAMA_KV_CACHE_TYPE\"}],\" Values\"]}]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"q8_0\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"q4_0\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"f16\"}],\" (for 16-bit floating point caching)\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"These settings can help optimize memory usage and speed when running large models locally.\"}]]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"17:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/b71c1cfea9076b4b.js\"],\"IconMark\"]\n10:[[\"$\",\"title\",\"0\",{\"children\":\"Tweaking GenAI LLMs locally using Ollama\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A quick reference guide for running GenAI LLM models and tweaking them using Ollama.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L17\",\"3\",{}]]\nc:null\n"])</script></body></html>